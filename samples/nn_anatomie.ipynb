{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.417022   0.72032449 0.00011437 0.30233257]\n [0.14675589 0.09233859 0.18626021 0.34556073]\n [0.39676747 0.53881673 0.41919451 0.6852195 ]]\n[[0.20445225]\n [0.87811744]\n [0.02738759]\n [0.67046751]]\n[[0.59791076 0.63153712 0.60329049 0.66490264]]\n[[ 0.00760395]\n [ 0.67019843]\n [-0.17123186]\n [ 0.45156368]]\n[[0.75751918]]\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "# Neuronale Netze: Anatomie #\n",
    "#############################\n",
    "\n",
    "# Im Folgenden betrachten wir ein ganz einfaches Beispiel eines Neuronalen Netzes - keine kompexere Variante wie z.B. rekurrente Netze (mit Gewichten innerhalb einer Schicht oder gar zu vorherigen Schichte, und keine Deep Learning-Netze (mit mehr als einer Zwischenschicht). Des Weiteren verzichten wir auf die Verwendung eines sog. Bias und einer sog. Lernarte, die normalerweise ebenfalls zu Neuronalen Netzen gehören. Die Vorgehensweise eines Neuronalen Netzes ist - je komplexer es wird - immer noch weniger vom Nutzer nachvollziehbar, bzw. ist schlicht in der Entwicklung der Lösung eine \"black box\". Wir fokussieren uns daher auf ein ganz einfaches Beispiel, um eine Idee davon zu bekommen, was in einem Neuronalen Netz passiert. Wir werden keinen vorgefertigten NN-Algorithmus zur Validierung unseres eigenen Codes verwenden - ein so \"einfaches\" Beispiel ist damit kaum zu replizieren, und diese Beispiel ist ohnehin bereits komplex genug. \n",
    "\n",
    "import numpy as np\n",
    "import random \n",
    "\n",
    "# Um die Ergebnisse wieder replizierbar zu machen, verwenden wir erneut Pseudo-Zufallszahlen (diese werden benötigt, da die initialen Gewichte irgendwie gesetzt werden müssen).\n",
    "np.random.seed(1)\n",
    "\n",
    "# Unsere Aktivierungsfunktion (also die Funktion, die das Eingangssignal eines Neurons zum tatsächlichen Wert des Neurons umwandelt) ist die Sigmoidfunktion: f(z)=1/(1+e^(-z)). z ist hierbei das Eingangssignal.\n",
    "def sigmoid(X):\n",
    "    return 1/(1 + np.exp(-X))\n",
    "\n",
    "# Für die Bestimmung der backpropagation-Gewichte brauchen wir u.a. die partielle Ableitung der Sigmoid-Funktion (Herleitung ist hier nicht nötig).\n",
    "def ableitung_sigmoid(X):\n",
    "    return sigmoid(X) * (1 - sigmoid(X))\n",
    "\n",
    "# Definieren wir eine Klasse für das Neuronale Netz, mit all seinen Eigenschaften:\n",
    "class NeuronalesNetz:\n",
    "    def __init__(self, X, Y):\n",
    "        # Input-Daten in der Inputschicht\n",
    "        self.input = X\n",
    "        # Output-Daten\n",
    "        self.output = Y\n",
    "        # Gewichte von der Inputschicht zur Zwischenschicht (das sind 3 Inputschicht-Neuronen mal 4 Zwischenschichten-Neuronen = 12 Gewichte)\n",
    "        self.gewichte_input_zwischen = np.random.random_sample((self.input.shape[1],4))\n",
    "        # Gewicht von der Zwischenschicht zur Outputschicht (das sind 4 Zwischenschichten-Neuronen mal 1 Outputschicht-Neuron = 4 Gewichte)\n",
    "        self.gewichte_zwischen_output = np.random.random_sample((4,1))                 \n",
    "        # Vorhersage des Neuronalen Netzes in der Outputschicht, zunächst mit Nullern befüllt\n",
    "        self.output_vorhersage = np.zeros(self.output.shape)\n",
    "\n",
    "\n",
    "    # Nun müssen wir den feedforward-Prozess programmieren, sprich die Berechnung der Signale z:\n",
    "    def feedforward(self):\n",
    "        # Die Werte der 4 Zwischenschicht-Neuronen werden berechnet mit sigmoid(z)=1/(1+e^-z), mit z=W*x, wobei W die Gewichte von der Input- zur Zwischenschicht sind, und x die Inputdaten\n",
    "        self.werte_zwischen = sigmoid(np.dot(self.input, self.gewichte_input_zwischen))\n",
    "        # Der Wert des Output-Neurons wird ebenfalls berechnet mit sigmoid(z)=1/(1+e^-z), mit z=W*x, wobei W diesmal Gewichte von der Zwischen- zur Outputschicht sind, und x die zuvor berechneten Werte der 4 Zwischenschicht-Neuronen\n",
    "        self.output_vorhersage = sigmoid(np.dot(self.werte_zwischen, self.gewichte_zwischen_output))\n",
    "\n",
    "    # Nun müssen wir den backpropagation-Prozess programmieren, sprich die Berechnung der Veränderung der Gewichte, sodass der Verlust minimal wird\n",
    "    def backpropagation(self):\n",
    "\n",
    "        # Die Anpassung der 4 Gewichte von der Zwischen- zur Outputschicht haben wir explizit von Hand berechnet: Die Ableitung der Verlustfunktion nach den Gewichten ist das Produkt aus drei einzelnen partiellen Ableitungen: 1. die Ableitung der Verlustfunktion nach dem vorhergesagten Output (2*(self.output - self.output_vorhersage)), 2. die Ableitung des vorhergesagten Outputs nach dem im Output-Neuron ankommenden Signal - also nichts anderes als die Ableitung der Sigmoid-Funktion (ableitung_sigmoid(self.output)), und 3. die Ableitung des im Output-Neuron ankommenden Signals nach den Gewichten von der Zwischenschicht zur Outputschicht (self.werte_zwischen.T):\n",
    "        ableitung_gewichte_zwischen_output = np.dot(self.werte_zwischen.T, (2*(self.output - self.output_vorhersage) * ableitung_sigmoid(self.output_vorhersage)))\n",
    "        # Die Anpassung der 12 Gewichte von der Input- zur Zwischenschicht wird auf die selbe Weise berechnet, haben wir aber aus Zeitgründen nicht explizit gemacht:\n",
    "        ableitung_gewichte_input_zwischen = np.dot(self.input.T,  (np.dot(2*(self.output - self.output_vorhersage) * ableitung_sigmoid(self.output_vorhersage), self.gewichte_zwischen_output.T) * ableitung_sigmoid(self.werte_zwischen)))\n",
    "\n",
    "        # Nun müssen die Gewichte überschrieben werden: Die neuen Gewichte sind die alten Gewichte plus der Verändung der Gewicht, wie sie genau durch die Ableitung repräsentiert wird.\n",
    "        self.gewichte_zwischen_output = self.gewichte_zwischen_output + ableitung_gewichte_zwischen_output\n",
    "        self.gewichte_input_zwischen = self.gewichte_input_zwischen + ableitung_gewichte_input_zwischen\n",
    "# Nun erzeugen wir Daten. Der Einfachheit halber nur einen Datensatz mit drei Input-Werten (für jedes Neuron in der Inputschicht einen), und einen Output-Wert. Ihr könnt hier beliebige viele Datenwerte noch hinzufügen!\n",
    "if __name__ == \"__main__\":\n",
    "    X = np.array([[0,0,1]])\n",
    "    Y = np.array([[0]])\n",
    "\n",
    "    # Nun erzeugen wir ein Objekt der Klasse Neuonales Netzwerk:\n",
    "    NN = NeuronalesNetz(X,Y)\n",
    "\n",
    "    # Nun machen wir erstmal lediglich eine Iteration der for-Schleife, um unsere von-Hand-Berechnungen zu verifizieren. Ihr könnt später gerne range() auf 1.000 oder höher stellen, um zu sehen, dass die Vorhersage des Y-Werts 0 immer genauer wird!\n",
    "\n",
    "    # Für die von-Hand-Berechnung können wir uns hier (also nach der Erzeugung des Objekts, aber noch vor feedforward und backpropagation) die initialen, pseudo-zufällig gewählten Gewichte holen. Davor zwingen wir Python noch zur normalen Formatierung von Kommazahlen:\n",
    "    np.set_printoptions(suppress=True)\n",
    "    print(NN.gewichte_input_zwischen)\n",
    "    print(NN.gewichte_zwischen_output)\n",
    "    for i in range(1):\n",
    "        NN.feedforward()\n",
    "        NN.backpropagation()\n",
    "\n",
    "    # Zur Sicherstellung können die von Hand gerechneten Werte der Zwischenschicht-Neuronen hiermit überprüft werden:\n",
    "    print(NN.werte_zwischen)\n",
    "\n",
    "    # Ebenso kann überprüft werden, ob die neuen Gewichte von der Zwischen- zur Outputschicht richtig berechnet wurden:\n",
    "    print(NN.gewichte_zwischen_output)\n",
    "\n",
    "        # Nun können wir uns danach die Vorhersage ausgeben. Diese ist ca. 0,7575 (also noch weit weg von der tatsächlichen 0, was nicht überraschend ist, weil der erste Durchlauf noch mit zufälligen, nicht-verbesserten Gewichten durchgeführt wurde). Hier kann ebenfalls überprüft werden, ob die von-Hand-Berechnung der Vorhersage des Neuronalen Netzes richtig ist:\n",
    "    print(NN.output_vorhersage)"
   ]
  }
 ]
}